{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2 по обработке текстов\n",
    "\n",
    "Рассмотрим задачу бинарной классификации. Пусть дано два списка имен: мужские и женские имена. Требуется разработать классификатор, который по данному имени будет определять мужское оно или женское.\n",
    "\n",
    "Данные: \n",
    "* Женские имена: female.txt\n",
    "* Мужские имена: male.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Предварительная обработка данных\n",
    "\n",
    "1. Удалите неоднозначные имена (те имена, которые являются и мужскими, и женскими дновременно), если такие есть; \n",
    "2. Создайте обучающее и тестовое множество так, чтобы в обучающем множестве классы были сбалансированы, т.е. к классу принадлежало бы одинаковое количество имен;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Часть 2. Базовый метод классификации\n",
    "\n",
    "Используйте метод наивного Байеса или логистическую регрессию для классификации имен: в качестве признаков используйте символьные $n$-граммы. Сравните результаты, получаемые при разных $n=2,3,4$ по $F$-мере и аккуратности. В каких случаях метод ошибается?\n",
    "\n",
    "Для генерации $n$-грамм используйте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Часть 3. Нейронная сеть\n",
    "\n",
    "\n",
    "Используйте  реккурентную нейронную сеть с  LSTM для решения задачи. В ней может быть несколько слоев с LSTM, несколько слоев c Bidirectional(LSTM).  У нейронной сети один выход, определяющий класс имени. \n",
    "\n",
    "Представление имени для классификации в этом случае: бинарная матрица размера (количество букв в алфавите $\\times$ максимальная длина имени). Обозначим его через $x$. Если первая буква имени a, то $x[1][1] = 1$, если вторая – b, то  $x[2][1] = 1$.  \n",
    "\n",
    "Не забудьте про регуляризацию нейронной сети дропаутами. \n",
    "\n",
    "Сравните результаты классификации разными методами. Какой метод лучше и почему?\n",
    "\n",
    "Сравните результаты, получаемые при разных значениях дропаута, разных числах узлов на слоях нейронной сети по $F$-мере и аккуратности. В каких случаях нейронная сеть ошибается?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если совсем не получается запрограммировать нейронную сеть самостоятельно, обратитесь к туториалу тут: https://github.com/divamgupta/lstm-gender-predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предварительная обработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузим и почистим имена"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"female.txt\") as f:\n",
    "    fnames = f.readlines()\n",
    "    fnames = [ name.strip() for name in fnames ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"male.txt\") as f:\n",
    "    mnames = f.readlines()\n",
    "    mnames = [ name.strip() for name in mnames ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', 'Abby', 'Abigael', 'Abigail', 'Abigale']\n",
      "fnames len:  5001\n",
      "['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot', 'Abbott', 'Abby', 'Abdel', 'Abdul', 'Abdulkarim']\n",
      "mnames len:  2943\n"
     ]
    }
   ],
   "source": [
    "print(fnames[:10])\n",
    "print('fnames len: ', len(fnames))\n",
    "print(mnames[:10])\n",
    "print('mnames len: ', len(mnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fnames_cleared = [x for x in fnames if x not in mnames]\n",
    "np.random.shuffle(fnames_cleared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnames_cleared = [x for x in mnames if x not in fnames]\n",
    "np.random.shuffle(mnames_cleared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nessie', 'Anetta', 'Audi', 'Dorry', 'Ardis', 'Lillis', 'Kelli', 'Harli', 'Anna-Diane', 'Simonne']\n",
      "fnames len:  4635\n",
      "['Sanford', 'Parnell', 'Giff', 'Barth', 'Giraldo', 'Teador', 'Pepillo', 'Kingsly', 'Cesar', 'Dougie']\n",
      "mnames len:  2578\n"
     ]
    }
   ],
   "source": [
    "print(fnames_cleared[:10])\n",
    "print('fnames len: ', len(fnames_cleared))\n",
    "print(mnames_cleared[:10])\n",
    "print('mnames len: ', len(mnames_cleared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2062\n"
     ]
    }
   ],
   "source": [
    "coef = int(len(mnames_cleared) * 0.8)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4635\n",
      "2578\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "fzeros = np.zeros(len(fnames_cleared))\n",
    "mzeros = np.ones(len(mnames_cleared))\n",
    "print(len(fzeros))\n",
    "print(len(mzeros))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим мужские и женские имена, одновременно присвоив им метку пола"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Nessie' '0.0']\n",
      " ['Anetta' '0.0']\n",
      " ['Audi' '0.0']\n",
      " ['Dorry' '0.0']\n",
      " ['Ardis' '0.0']\n",
      " ['Lillis' '0.0']\n",
      " ['Kelli' '0.0']\n",
      " ['Harli' '0.0']\n",
      " ['Anna-Diane' '0.0']\n",
      " ['Simonne' '0.0']]\n",
      "(2062, 2)\n",
      "[['Amargo' '0.0']\n",
      " ['Alia' '0.0']\n",
      " ['Casie' '0.0']\n",
      " ['Blinny' '0.0']\n",
      " ['Roxane' '0.0']\n",
      " ['Joyan' '0.0']\n",
      " ['Sharyl' '0.0']\n",
      " ['Angelique' '0.0']\n",
      " ['Cicily' '0.0']\n",
      " ['Farand' '0.0']]\n",
      "(2573, 2)\n"
     ]
    }
   ],
   "source": [
    "fnames_train = np.vstack((fnames_cleared[:coef], fzeros[:coef])).T\n",
    "print(fnames_train[:10])\n",
    "print(fnames_train.shape)\n",
    "fnames_test = np.vstack((fnames_cleared[coef : ], fzeros[coef : ])).T\n",
    "print(fnames_test[:10])\n",
    "print(fnames_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Nessie' '0.0']\n",
      " ['Anetta' '0.0']\n",
      " ['Audi' '0.0']\n",
      " ['Dorry' '0.0']\n",
      " ['Ardis' '0.0']\n",
      " ['Lillis' '0.0']\n",
      " ['Kelli' '0.0']\n",
      " ['Harli' '0.0']\n",
      " ['Anna-Diane' '0.0']\n",
      " ['Simonne' '0.0']]\n",
      "(2062, 2)\n",
      "[['Ambrosius' '1.0']\n",
      " ['Hershel' '1.0']\n",
      " ['Sander' '1.0']\n",
      " ['Renaud' '1.0']\n",
      " ['Elwyn' '1.0']\n",
      " ['Walt' '1.0']\n",
      " ['Andrzej' '1.0']\n",
      " ['Robb' '1.0']\n",
      " ['Engelbart' '1.0']\n",
      " ['Cyrille' '1.0']]\n",
      "(516, 2)\n"
     ]
    }
   ],
   "source": [
    "mnames_train = np.vstack((mnames_cleared[:coef], mzeros[:coef])).T\n",
    "print(fnames_train[:10])\n",
    "print(fnames_train.shape)\n",
    "mnames_test = np.vstack((mnames_cleared[coef : ], mzeros[coef : ])).T\n",
    "print(mnames_test[-10:])\n",
    "print(mnames_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Tomasina' '0.0']\n",
      " ['Consuela' '0.0']\n",
      " ['Maryanna' '0.0']\n",
      " ['Mellisa' '0.0']\n",
      " ['Donella' '0.0']\n",
      " ['Gwenny' '0.0']\n",
      " ['Jacquette' '0.0']\n",
      " ['Darleen' '0.0']\n",
      " ['Cathryn' '0.0']\n",
      " ['Tiertza' '0.0']\n",
      " ['Sanford' '1.0']\n",
      " ['Parnell' '1.0']\n",
      " ['Giff' '1.0']\n",
      " ['Barth' '1.0']\n",
      " ['Giraldo' '1.0']\n",
      " ['Teador' '1.0']\n",
      " ['Pepillo' '1.0']\n",
      " ['Kingsly' '1.0']\n",
      " ['Cesar' '1.0']\n",
      " ['Dougie' '1.0']]\n",
      "(4124, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.append(fnames_train, mnames_train, axis=0)\n",
    "X_test = np.append(fnames_test, mnames_test, axis=0)\n",
    "print(X_train[coef - 10 : coef + 10])\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще раз перемешиваем все имена"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(X_train)\n",
    "np.random.shuffle(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Camile' '0.0']\n",
      " ['Leia' '0.0']\n",
      " ['Spiro' '1.0']\n",
      " ['Laurella' '0.0']\n",
      " ['Daniella' '0.0']\n",
      " ['Stephani' '0.0']\n",
      " ['Pieter' '1.0']\n",
      " ['Anjela' '0.0']\n",
      " ['Berta' '0.0']\n",
      " ['Edouard' '1.0']\n",
      " ['Ben' '1.0']\n",
      " ['Cornelius' '1.0']\n",
      " ['Ruben' '1.0']\n",
      " ['Fletch' '1.0']\n",
      " ['Xenos' '1.0']\n",
      " ['Cosmo' '1.0']\n",
      " ['Claresta' '0.0']\n",
      " ['Archy' '1.0']\n",
      " ['Marchall' '1.0']\n",
      " ['Vanni' '0.0']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[coef - 10 : coef + 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция вывода метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "def metrics_test(predicted, target):\n",
    "    acc = accuracy_score(predicted, target)\n",
    "    micro_f1 = f1_score(predicted, target, average = 'micro')\n",
    "    micro_p = f1_score(predicted, target, average = 'micro')\n",
    "    micro_r = f1_score(predicted, target, average = 'micro')\n",
    "    macro_f1 = f1_score(predicted, target, average = 'macro')\n",
    "    macro_p = f1_score(predicted, target, average = 'macro')\n",
    "    macro_r = f1_score(predicted, target, average = 'macro')\n",
    "    print('acc={0:1.4f}'.format(acc))\n",
    "    print('micro F1={0:1.4f}, micro P={1:1.4f}, micro R={2:1.4f}'.format(micro_f1, micro_p, micro_r))\n",
    "    print('macro F1={0:1.4f}, macro P={1:1.4f}, macro R={2:1.4f}\\n'.format(macro_f1, macro_p, macro_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Базовый метод классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Derron', 'Boyd', 'Tre', ..., 'Lincoln', 'Greggory', 'Pierrette'],\n",
       "      dtype='<U32')"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим массив содержащий в строках имена в виде биграмм (по аналогии с предложениями или целыми текстами. Только в текстах содержались слова, а мы будем работать, как будто бы Имя - Предложение, Биграмма - Слово в предложении)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "def get_ngrams(train, ngrams_count = 2):\n",
    "    sequences = []\n",
    "    for words, sex in train[:]:\n",
    "        seq = list(words.lower())\n",
    "        bigrams = ngrams(seq, ngrams_count)\n",
    "        bi = []\n",
    "        for grams in bigrams:           \n",
    "            if len(grams) >= ngrams_count:\n",
    "#                 print(\"{} : n {}\".format(len(grams), ngrams_count))\n",
    "                bi.append(\"\".join(grams))\n",
    "        sequences.append([\" \".join(bi), int(float(sex))])\n",
    "    return np.array(sequences)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на предсказания используя Биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gram = get_ngrams(X_train)\n",
    "X_test_gram = get_ngrams(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4124, 425)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train_gram[:,0])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_counts, X_train_gram[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_fin = count_vect.transform(X_test_gram[:,0])\n",
    "predicted = clf.predict(X_test_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.7614\n",
      "micro F1=0.7614, micro P=0.7614, micro R=0.7614\n",
      "macro F1=0.6709, macro P=0.6709, macro R=0.6709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_test(predicted, X_test_gram[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на предсказания используя Три-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gram = get_ngrams(X_train, ngrams_count=3)\n",
    "X_test_gram = get_ngrams(X_test, ngrams_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4124, 2680)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train_gram[:,0])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_counts, X_train_gram[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_fin = count_vect.transform(X_test_gram[:,0])\n",
    "predicted = clf.predict(X_test_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.8012\n",
      "micro F1=0.8012, micro P=0.8012, micro R=0.8012\n",
      "macro F1=0.7144, macro P=0.7144, macro R=0.7144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_test(predicted, X_test_gram[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на предсказания используя Четыре-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_train_gram = get_ngrams(X_train, ngrams_count=4)\n",
    "X_test_gram = get_ngrams(X_test, ngrams_count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['derr erro rron', '1'],\n",
       "       ['boyd', '1'],\n",
       "       ['', '1'],\n",
       "       ..., \n",
       "       ['linc inco ncol coln', '1'],\n",
       "       ['greg regg eggo ggor gory', '1'],\n",
       "       ['pier ierr erre rret rett ette', '0']],\n",
       "      dtype='<U59')"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4124, 5738)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train_gram[:,0])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_counts, X_train_gram[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_fin = count_vect.transform(X_test_gram[:,0])\n",
    "predicted = clf.predict(X_test_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.8249\n",
      "micro F1=0.8249, micro P=0.8249, micro R=0.8249\n",
      "macro F1=0.7225, macro P=0.7225, macro R=0.7225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_test(predicted, X_test_gram[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем максимальные значения для того что бы понять размерности входных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalEntries = X_train.shape[0]\n",
    "maxlen = len(max( X_train[:, 0] , key=len)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим таблицы для перевода символов в индексы и обратно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set(  \"\".join(X_train[:, 0])  )\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на размерности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total endtries  4124\n",
      "max len  15\n",
      "total chars: 54\n"
     ]
    }
   ],
   "source": [
    "print(\"total endtries \" , totalEntries)\n",
    "print(\"max len \" , maxlen)\n",
    "print('total chars:', len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим пустые массивы нужной нам размерности что бы после заполнить их данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((totalEntries , maxlen, len(chars) ), dtype=np.bool)\n",
    "y = np.zeros((totalEntries , 2 ), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним данными подготовленные массивы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Derron', '1.0'],\n",
       "       ['Boyd', '1.0'],\n",
       "       ['Tre', '1.0'],\n",
       "       ..., \n",
       "       ['Lincoln', '1.0'],\n",
       "       ['Greggory', '1.0'],\n",
       "       ['Pierrette', '0.0']],\n",
       "      dtype='<U32')"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот тут у нас фактически one_hot_encoding. Я взял с примера, хотя есть подозрение что можно просто прикрутить стандартный, предварительно разбив все придложения (в нашем случае имена) на массивы символов и скормить стандартному (хоть из sklearn хоть из keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, sex) in enumerate(X_train):\n",
    "    for t, char in enumerate(name):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, int(float(sex))] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим модель сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4124/4124 [==============================] - 49s 12ms/step - loss: 0.5829\n",
      "Epoch 2/20\n",
      "4124/4124 [==============================] - 51s 12ms/step - loss: 0.5141\n",
      "Epoch 3/20\n",
      "4124/4124 [==============================] - 50s 12ms/step - loss: 0.4890\n",
      "Epoch 4/20\n",
      "4124/4124 [==============================] - 49s 12ms/step - loss: 0.4692\n",
      "Epoch 5/20\n",
      "4124/4124 [==============================] - 50s 12ms/step - loss: 0.4384\n",
      "Epoch 6/20\n",
      "4124/4124 [==============================] - 51s 12ms/step - loss: 0.4126\n",
      "Epoch 7/20\n",
      "4124/4124 [==============================] - 52s 13ms/step - loss: 0.3905\n",
      "Epoch 8/20\n",
      "4124/4124 [==============================] - 50s 12ms/step - loss: 0.3789\n",
      "Epoch 9/20\n",
      "4124/4124 [==============================] - 50s 12ms/step - loss: 0.3507\n",
      "Epoch 10/20\n",
      "4124/4124 [==============================] - 49s 12ms/step - loss: 0.3319\n",
      "Epoch 11/20\n",
      "4124/4124 [==============================] - 52s 13ms/step - loss: 0.3249\n",
      "Epoch 12/20\n",
      "4124/4124 [==============================] - 49s 12ms/step - loss: 0.3092\n",
      "Epoch 13/20\n",
      "4124/4124 [==============================] - 49s 12ms/step - loss: 0.2883\n",
      "Epoch 14/20\n",
      "4124/4124 [==============================] - 49s 12ms/step - loss: 0.2694\n",
      "Epoch 15/20\n",
      "4124/4124 [==============================] - 48s 12ms/step - loss: 0.2506\n",
      "Epoch 16/20\n",
      "4124/4124 [==============================] - 50s 12ms/step - loss: 0.2387\n",
      "Epoch 17/20\n",
      "4124/4124 [==============================] - 49s 12ms/step - loss: 0.2206 \n",
      "Epoch 18/20\n",
      "4124/4124 [==============================] - 50s 12ms/step - loss: 0.2070\n",
      "Epoch 19/20\n",
      "4124/4124 [==============================] - 50s 12ms/step - loss: 0.1856\n",
      "Epoch 20/20\n",
      "4124/4124 [==============================] - 50s 12ms/step - loss: 0.1709\n",
      "4124/4124 [==============================] - 12s 3ms/step\n",
      "score  0.132893151216\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=16, epochs=20)\n",
    "model.save_weights('my_model_weights.h5')\n",
    "score = model.evaluate(X, y, batch_size=16)\n",
    "print(\"score \" , score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем максимальные значения для того что бы понять размерности входных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalEntries_t = X_test.shape[0]\n",
    "maxlen_t = len(max( X_test[:, 0] , key=len)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим пустые массивы нужной нам размерности что бы после заполнить их данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = np.zeros((totalEntries_t , maxlen_t, len(chars) ), dtype=np.bool)\n",
    "y_t = np.zeros((totalEntries_t , 2 ), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим тестовые данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Не найден символ ' в имени: E'Lane\n"
     ]
    }
   ],
   "source": [
    "for i, (name, sex) in enumerate(X_test):\n",
    "    for t, char in enumerate(name):\n",
    "        try:\n",
    "            X_t[i, t, char_indices[char]] = 1\n",
    "        except:\n",
    "            print(\"Не найден символ {} в имени: {}\".format(char, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3089/3089 [==============================] - 6s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict_classes(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tt = X_test[:,1].astype(float).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.8330\n",
      "micro F1=0.8330, micro P=0.8330, micro R=0.8330\n",
      "macro F1=0.7666, macro P=0.7666, macro R=0.7666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_test(predicted, X_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Нейронку обучать долго, но результаты лучше.\n",
    "### 2) Чем больше размерность n-грамм, тем лучше результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
